---
layout: publication
title: "On the Naturalness of Software"
authors: Abram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su, Premkumar Devanbu
conference: ICSE
year: 2012
tags: ["language model", "autocomplete"]
---
Natural languages like English are rich, complex,
and powerful. The highly creative and graceful use of languages
like English and Tamil, by masters like Shakespeare and
Avvaiyar, can certainly delight and inspire. But in practice,
given cognitive constraints and the exigencies of daily life, most
human utterances are far simpler and much more repetitive
and predictable. In fact, these utterances can be very usefully
modeled using modern statistical methods. This fact has led
to the phenomenal success of statistical approaches to speech
recognition, natural language translation, question-answering,
and text mining and comprehension.

We begin with the conjecture that most software is also
natural, in the sense that it is created by humans at work,
with all the attendant constraints and limitations—and thus,
like natural language, it is also likely to be repetitive and
predictable. We then proceed to ask whether a) code can
be usefully modeled by statistical language models and b)
such models can be leveraged to support software engineers.
Using the widely adopted n-gram model, we provide empirical
evidence supportive of a positive answer to both these questions.
We show that code is also very repetitive, and in fact even more
so than natural languages. As an example use of the model,
we have developed a simple code completion engine for Java
that, despite its simplicity, already improves Eclipse’s built-in
completion capability. We conclude the paper by laying out a
vision for future research in this area.


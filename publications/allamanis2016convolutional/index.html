<!DOCTYPE html>
<html lang="en-us">

  <head>
  <!-- Global Site Tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-107339008-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments)};
    gtag('js', new Date());
    gtag('config', 'UA-107339008-1');
  </script>

  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="keywords" content="machine learning, source code, big code, naturalness, software engineering, programming languages">

  <title>
    
      A Convolutional Attention Network for Extreme Summarization of Source Code &middot; Machine Learning for Big Code and Naturalness
    
  </title>

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="ML4Code" />

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.10.16/js/jquery.dataTables.min.js"></script>
</head>


  <body class="theme-base-0d layout-reverse">

    <a href='/contributing.html' class='ribbon'>Contribute to ML4Code</a>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Machine Learning for Big Code and Naturalness
        </a>
      </h1>
      <p class="lead">Research on machine learning for source code.</p>      
    </div>

  <nav class="sidebar-nav">
   <div class="sidebar-item"><p style="font-size: 12px">Search related work <input type='text' id='searchTarget' size="16"/> <button onClick="search();">Go</button></p></div>
   <a class="sidebar-nav-item" href="/papers.html">List of Papers</a>
   <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
   <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a>
   <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a>

   <a class="sidebar-nav-item" href="/base-taxonomy/">Core Taxonomy</a>


  <a class="sidebar-nav-item" href="/resources.html">Resources, Courses &#38; Events</a>
  <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
  </nav>

  <div class="sidebar-item">
    <p style="font-size: 12px">Contact <a href="https://miltos.allamanis.com">Miltos Allamanis</a> about this survey or website.
    <span style="font-size: 9px">
      Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>.
    </span></p>
  </div>
</div></div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      <div class="page">
  <h1 class="page-title">A Convolutional Attention Network for Extreme Summarization of Source Code</h1>
  <h5>M. Allamanis, H. Peng, C. Sutton. ICML 2016</h5>
  <p>
    
      [<a href="http://groups.inf.ed.ac.uk/cup/codeattention/" target="_blank">website</a>]
    
      [<a href="https://github.com/mast-group/convolutional-attention" target="_blank">code</a>]
    
      [<a href="http://jmlr.org/proceedings/papers/v48/allamanis16.pdf" target="_blank">proceedings</a>]
    
      [<a href="http://techtalks.tv/talks/a-convolutional-attention-network-for-extreme-summarization-of-source-code/62461/" target="_blank">presentation video</a>]
    
      [<a href="http://gitxiv.com/posts/A6HFFyK7CmNLaSjG7/a-convolutional-attention-network-for-extreme-summarization" target="_blank">GitXiV</a>]
    
    &nbsp;<a href='http://scholar.google.com/scholar?q=A Convolutional Attention Network for Extreme Summarization of Source Code' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
    &nbsp;<a href='https://www.semanticscholar.org/search?q=A Convolutional Attention Network for Extreme Summarization of Source Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
    &nbsp;<a href='http://academic.microsoft.com/#/search?iq=A%20Convolutional%20Attention%20Network%20for%20Extreme%20Summarization%20of%20Source%20Code' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/ms-academic.png"/></a>
    <br/>
    
      <tag><a href="/tags.html#naming">naming</a></tag>
    
      <tag><a href="/tags.html#summarization">summarization</a></tag>
    
  </p>
      
      
      <p><p>Attention mechanisms in neural networks have proved useful for problems in which
the input and output do not have fixed dimension. Often there exist features that
are locally translation invariant and would be valuable for directing the model’s attention,
but previous attentional architectures are not constructed to learn such features specifically.
We introduce an attentional neural network that employs convolution on the input tokens to detect
local time-invariant and long-range topical attention features in a context-dependent way. We
apply this architecture to the problem of extreme summarization of source code snippets into short,
descriptive function name-like summaries. Using those features, the model sequentially generates a
summary by marginalizing over two attention mechanisms: one that predicts the next summary token based 
n the attention weights of the input tokens and another that is able to copy a code token as-is directly
into the summary. We demonstrate our convolutional attention neural network’s performance on 10 popular Java
projects showing that it achieves better performance compared to previous attentional mechanisms.</p>
</p>
</div>

    </div>

  </body>
</html>
